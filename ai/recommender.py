# ai/recommender.py
import streamlit as st
import google.generativeai as genai
import os
from typing import List, Dict, Set
import json
from dotenv import load_dotenv
import requests 
from bs4 import BeautifulSoup 
from ai.prompts import (
    get_segment_recommendation_prompt, 
    get_segment_filtering_prompt,
    get_expansion_and_understanding_prompt
)
import pandas as pd
import time # 429 ì˜¤ë¥˜(ì¬ì‹œë„/ì§€ì—°) ë°©ì§€ë¥¼ ìœ„í•´ time ì„í¬íŠ¸
import re # í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•´ re ì„í¬íŠ¸

load_dotenv()

class AISegmentRecommender:
    def __init__(self, data_manager):
        self.data_manager = data_manager
        self.segments_data = data_manager.load_segments()
        self.api_key = os.getenv('GEMINI_API_KEY')
        self.model = None
        self.gemini_available = False
        self._initialize_gemini()
    
    def _initialize_gemini(self):
        if not self.api_key:
            st.error("âŒ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return
        try:
            genai.configure(api_key=self.api_key)
            try:
                # 1ìˆœìœ„: 'gemini 2.5-flash' (ì‚¬ì¥ë‹˜ ëª©ë¡ì˜ 'models/gemini-flash-latest')
                self.model = genai.GenerativeModel('models/gemini-flash-latest')
            except:
                # 2ìˆœìœ„: 'pro' (ì‚¬ì¥ë‹˜ ëª©ë¡ì˜ 'models/gemini-pro-latest')
                self.model = genai.GenerativeModel('models/gemini-pro-latest')
            self.gemini_available = True
            
            # [â˜…ìˆ˜ì •] ìš”ì²­ì‚¬í•­ 1: "AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ" ë©”ì‹œì§€ ë³µì› (ì •í™•í•œ ëª¨ë¸ëª… í‘œì‹œ)
            st.success(f"âœ… AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.model.model_name}")
            
        except Exception as e:
            st.error(f"âŒ Gemini API ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            st.error("ai/recommender.py 32~37 ë¼ì¸ì˜ ëª¨ë¸ ì´ë¦„ì„ check_models.py ëª©ë¡ì„ ì°¸ê³ í•˜ì—¬ ìˆ˜ì •í•˜ì„¸ìš”.")
            self.gemini_available = False

    def _generate_with_retry(self, prompt: str, max_retries: int = 3) -> str:
        """
        Gemini API í˜¸ì¶œ ì‹œ 429 ì˜¤ë¥˜(í• ë‹¹ëŸ‰)ê°€ ë°œìƒí•˜ë©´
        ìë™ìœ¼ë¡œ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ (Exponential Backoff)
        """
        retries = 0
        while retries < max_retries:
            try:
                response = self.model.generate_content(prompt)
                if not response or not response.text:
                    raise ValueError("Gemini APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
                return response.text # ì„±ê³µ ì‹œ ì‘ë‹µ í…ìŠ¤íŠ¸ ë°˜í™˜
            
            except Exception as e:
                if "429 Resource exhausted" in str(e) and retries < max_retries - 1:
                    retries += 1
                    wait_time = 2 ** retries 
                    st.warning(f"âš ï¸ API í• ë‹¹ëŸ‰(429) ì´ˆê³¼. {wait_time}ì´ˆ í›„ ì¬ì‹œë„... ({retries}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    raise e 
        
        raise Exception("API í• ë‹¹ëŸ‰ ì´ˆê³¼. ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨.")

    # [â˜…ìˆ˜ì •] 'AI í™•ì¥ í‚¤ì›Œë“œ' + "AIê°€ Aê¸‰ ì´ìœ  ìƒì„±" ë¡œì§ìœ¼ë¡œ ìˆ˜ì •
    def recommend_segments(self, product_name: str, website_url: str, num_recommendations: int = 3) -> (List[Dict], str, List[str]): # [â˜…ìˆ˜ì •] ë°˜í™˜ íƒ€ì… ë³€ê²½
        
        # [â˜…ìˆ˜ì •] ë¹„ì‹ë³„í™” ë°ì´í„° ì €ì¥ì„ ìœ„í•´ í•¨ìˆ˜ ì´ˆê¸°ì— ë°˜í™˜ ë³€ìˆ˜ ì„ ì–¸
        product_understanding = ""
        expanded_keywords = []

        if not product_name.strip() and not website_url.strip():
            st.error("âŒ 'ì œí’ˆëª…' ë˜ëŠ” 'ì œí’ˆ URL*'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return [], product_understanding, expanded_keywords # [â˜…ìˆ˜ì •]
            
        if not self.gemini_available or not self.model:
            st.error("âŒ Gemini AIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return [], product_understanding, expanded_keywords # [â˜…ìˆ˜ì •]
            
        # --- 0-1. URL ìŠ¤í¬ë˜í•‘ ---
        scraped_text = ""
        if website_url:
            with st.spinner(f"ğŸŒ {website_url} ì›¹í˜ì´ì§€ ë¶„ì„ ì¤‘..."):
                scraped_text = self._fetch_url_content(website_url)
                if not scraped_text:
                    st.warning("âš ï¸ ì›¹ì‚¬ì´íŠ¸ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ì½ì–´ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì œí’ˆëª…/URLë¡œë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")

        try:
            # 1. ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë¡œë“œ
            all_segments_info = self._get_available_segments_info()
            if not all_segments_info:
                st.error("âŒ ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (data/segments.json)")
                return [], product_understanding, expanded_keywords # [â˜…ìˆ˜ì •]
            
            # [â˜…ìˆ˜ì •] ìš”ì²­ì‚¬í•­ 3, 4: ë©”ì‹œì§€ ë³€ê²½ (ê°œìˆ˜ ì œê±°, ì‡¼ì‰ ê°•í™”)
            st.info(f"ğŸ” KOBATA AI íƒ€ê²Ÿ ë¶„ì„ ì—”ì§„ ê°€ë™...")

            # --- 0-2. AI ì œí’ˆ ì´í•´ + 'ìœ ì‚¬ í‚¤ì›Œë“œ í™•ì¥' (0ë‹¨ê³„) ---
            # product_understanding = "" # ë³€ìˆ˜ ì„ ì–¸ ìœ„ì¹˜ ìƒë‹¨ìœ¼ë¡œ ì´ë™
            # expanded_keywords = []
            with st.spinner("ğŸ¤– KOBATA AIê°€ ì œí’ˆì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ê³ , ì—°ê´€ íƒ€ê²Ÿì„ í™•ì¥í•©ë‹ˆë‹¤..."):
                try:
                    expansion_json = self._get_expansion_and_understanding(
                        product_name, website_url, scraped_text
                    )
                    product_understanding = expansion_json.get("product_understanding", "")
                    expanded_keywords = expansion_json.get("expanded_keywords", [])
                except Exception as e:
                    st.warning(f"âš ï¸ AI 0ë‹¨ê³„(í‚¤ì›Œë“œ í™•ì¥) ì‹¤íŒ¨: {e}")
            
            if not product_understanding:
                product_understanding = f"ì œí’ˆëª…: {product_name} (AI ìë™ ë¶„ì„ ì‹¤íŒ¨)"
                st.warning("AIê°€ ì œí’ˆì„ ìë™ìœ¼ë¡œ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì œí’ˆëª…ìœ¼ë¡œ ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")

            if product_name and product_name not in expanded_keywords:
                expanded_keywords.insert(0, product_name)

            st.info(f"**ğŸ’¡ AIê°€ ì´í•´í•œ ì œí’ˆ:** {product_understanding}")
            if expanded_keywords:
                 st.info(f"**ğŸ”‘ AIê°€ í™•ì¥í•œ ê²€ìƒ‰ í‚¤ì›Œë“œ:** {', '.join(expanded_keywords)}")

            # --- 1ë‹¨ê³„ (Python): 'ìš°ì„  ì¶”ì²œ í›„ë³´' (Aê¸‰) ì„ ë³„ ---
            priority_segments, remaining_segments = self._get_priority_segments(
                expanded_keywords, all_segments_info
            )
            
            # [â˜…ìˆ˜ì •] ìš”ì²­ì‚¬í•­ 2: "ìš°ì„  ì¶”ì²œ í›„ë³´" í™•ë³´ ë©”ì‹œì§€ ì œê±°

            # --- 2ë‹¨ê³„ (AI): 'Bê¸‰ í›„ë³´' í•„í„°ë§ ---
            num_b_class_needed = max(0, num_recommendations - len(priority_segments))
            num_to_filter = 20 
            b_class_candidates = []

            if remaining_segments and (num_b_class_needed > 0 or not priority_segments):
                with st.spinner(f"ğŸ¤– KOBATA AIê°€ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ DBì™€ 1ì°¨ ëŒ€ì¡°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤..."):
                    candidate_names = self._filter_with_gemini(
                        product_understanding, 
                        remaining_segments, 
                        num_to_filter=num_to_filter
                    )
                    b_class_candidates = self._get_segments_by_names(candidate_names, remaining_segments)
            
            # --- 3ë‹¨ê³„ (AI): Aê¸‰, Bê¸‰ ëª¨ë‘ ëª¨ì•„ ìµœì¢… ì¬ì •ë ¬ (AIê°€ ì´ìœ  ìƒì„±) ---
            final_candidate_list = priority_segments + b_class_candidates
            if not final_candidate_list:
                st.warning("âš ï¸ AIê°€ ì¶”ì²œ í›„ë³´ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.")
                final_candidate_list = all_segments_info[:20] 

            all_recommendations = []
            if final_candidate_list:
                time.sleep(1) # 429 ë°©ì§€
                with st.spinner(f"ğŸ¤– KOBATA AIê°€ í›„ë³´êµ°ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì •ë°€í•˜ê²Œ ì¬ì¡°ì •í•©ë‹ˆë‹¤..."):
                    ai_response = self._recommend_with_gemini(
                        product_understanding, 
                        final_candidate_list, 
                        num_to_recommend=max(num_recommendations, 5)
                    )
                    
                    if ai_response and ai_response.get("recommended_segments"):
                        all_recommendations = self._enrich_and_sort_segments(
                            ai_response.get("recommended_segments"), final_candidate_list
                        )
            
            # --- 4ë‹¨ê³„ (Python): ìµœì¢… ê²°í•© ë° Fallback ---
            final_recommendations = []
            seen_names = set()

            for seg in all_recommendations:
                if seg['name'] not in seen_names:
                    final_recommendations.append(seg)
                    seen_names.add(seg['name'])

            # Fallback ë¡œì§
            num_to_pad = num_recommendations - len(final_recommendations)
            if num_to_pad > 0:
                existing_names = {seg['name'] for seg in final_recommendations}
                fallback_segments = [seg for seg in all_segments_info if seg['name'] not in existing_names]
                
                for i in range(min(num_to_pad, len(fallback_segments))):
                    fallback_seg = fallback_segments[i].copy()
                    fallback_seg['reason'] = "ì œí’ˆê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ ê¸°ë³¸ ì„¸ê·¸ë¨¼íŠ¸ì…ë‹ˆë‹¤."
                    fallback_seg['confidence_score'] = 60 # ê¸°ë³¸ ì¶”ì²œ ì ìˆ˜
                    fallback_seg['key_factors'] = ["ê¸°ë³¸ ì¶”ì²œ"]
                    final_recommendations.append(fallback_seg)
            
            st.success(f"âœ… KOBATA AI íƒ€ê²Ÿ ë¶„ì„ ì™„ë£Œ!")
            
            # [â˜…ìˆ˜ì •] ë¹„ì‹ë³„í™” ë°ì´í„°ë¥¼ ë°˜í™˜
            return final_recommendations[:num_recommendations], product_understanding, expanded_keywords

        except Exception as e:
            st.error(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return [], product_understanding, expanded_keywords # [â˜…ìˆ˜ì •]
    
    def _fetch_url_content(self, url: str) -> str:
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                return meta_desc.get('content').strip()
            for tag in soup.find_all(['main', 'article']):
                text = tag.get_text(separator=' ', strip=True)
                if len(text) > 100:
                    return text[:1500]
            body_text = soup.body.get_text(separator=' ', strip=True)
            return body_text[:1500]
        except:
            return ""

    def _get_expansion_and_understanding(self, product_name: str, website_url: str, scraped_text: str) -> Dict:
        """ 0ë‹¨ê³„: ì œí’ˆ ì´í•´ + í‚¤ì›Œë“œ í™•ì¥ """
        prompt = get_expansion_and_understanding_prompt(
            product_name, website_url, scraped_text
        )
        try:
            raw_response_text = self._generate_with_retry(prompt)
        except Exception as e:
            st.error(f"âŒ Gemini API 0ë‹¨ê³„(í‚¤ì›Œë“œ í™•ì¥) í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {}
        try:
            cleaned_text = raw_response_text.strip().replace("```json\n", "").replace("\n```", "").strip()
            parsed_data = json.loads(cleaned_text)
            if not isinstance(parsed_data, dict) or "product_understanding" not in parsed_data:
                raise ValueError("AI ì‘ë‹µì´ 0ë‹¨ê³„ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return parsed_data
        except json.JSONDecodeError:
            st.error(f"âŒ AIê°€ 0ë‹¨ê³„ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤: {cleaned_text}")
            return {}
        except ValueError as e:
            st.error(f"âŒ AI 0ë‹¨ê³„ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return {}

    def _filter_with_gemini(self, product_understanding: str, remaining_segments: List[Dict], num_to_filter: int) -> List[str]:
        """ 1ë‹¨ê³„: í•„í„°ë§. Bê¸‰ í›„ë³´ ì„ ë³„ """
        if not remaining_segments or num_to_filter <= 0:
            return []
        
        segments_with_desc = [
            f"- {seg.get('name', 'N/A')} (ì„¤ëª…: {seg.get('description', 'N/A')})"
            for seg in remaining_segments
        ]
        segments_list_str = "\n".join(segments_with_desc)
        
        prompt = get_segment_filtering_prompt(
            product_understanding, 
            segments_list_str, 
            num_to_filter=num_to_filter
        )
        try:
            raw_response_text = self._generate_with_retry(prompt)
        except Exception as e:
            st.error(f"âŒ Gemini API 1ë‹¨ê³„(í•„í„°ë§) í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []
        try:
            cleaned_text = raw_response_text.strip().replace("```json\n", "").replace("\n```", "").strip()
            parsed_data = json.loads(cleaned_text)
            if not isinstance(parsed_data, dict) or "candidate_segments" not in parsed_data:
                raise ValueError("AI ì‘ë‹µì´ 1ë‹¨ê³„ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ('candidate_segments' í‚¤ ë¶€ì¬)")
            candidate_names = parsed_data.get("candidate_segments", [])
            if not isinstance(candidate_names, list):
                 raise ValueError("AI ì‘ë‹µ 'candidate_segments'ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return [str(name) for name in candidate_names] 
        except json.JSONDecodeError:
            st.error(f"âŒ AIê°€ 1ë‹¨ê³„ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤: {cleaned_text}")
            return []
        except ValueError as e:
            st.error(f"âŒ AI 1ë‹¨ê³„ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return []

    def _recommend_with_gemini(self, product_understanding: str, candidate_segments_info: List[Dict], num_to_recommend: int) -> Dict:
        """ 2ë‹¨ê³„: ì¬ì •ë ¬. A+B í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ìˆœìœ„ ê²°ì • """
        if not candidate_segments_info:
            return {}
        
        segments_with_desc = []
        for seg in candidate_segments_info: 
            seg_str = f"- {seg.get('name', 'N/A')} (ì„¤ëª…: {seg.get('description', 'N/A')}"
            advertisers = seg.get('recommended_advertisers')
            if advertisers and pd.notna(advertisers):
                clean_advertisers = str(advertisers).replace('\n', ', ')
                seg_str += f", ì¶”ì²œ ê´‘ê³ ì£¼: {clean_advertisers}"
            seg_str += ")"
            segments_with_desc.append(seg_str)
        
        segments_list_str = "\n".join(segments_with_desc)
        
        prompt = get_segment_recommendation_prompt(
            product_understanding, 
            segments_list_str, 
            num_to_recommend=num_to_recommend
        )
        try:
            raw_response_text = self._generate_with_retry(prompt)
        except Exception as e:
            st.error(f"âŒ Gemini API 2ë‹¨ê³„(ì¬ì •ë ¬) í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {}
        try:
            cleaned_text = raw_response_text.strip().replace("```json\n", "").replace("\n```", "").strip()
            parsed_data = json.loads(cleaned_text)
            if not isinstance(parsed_data, dict):
                raise ValueError("AI ì‘ë‹µì´ 2ë‹¨ê³„ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return parsed_data
        except json.JSONDecodeError:
            st.error(f"âŒ AIê°€ 2ë‹¨ê³„ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤: {cleaned_text}")
            return {}
    
    def _enrich_and_sort_segments(self, segments_from_ai: List[Dict], candidate_segments: List[Dict]) -> List[Dict]:
        """ 2ë‹¨ê³„ AI ì‘ë‹µ(A+B)ì„ ì •ë ¬ ë° ë³‘í•©í•˜ëŠ” í—¬í¼ """
        
        enriched_info_map = {
            s.get("name"): {
                "reason": s.get("reason", "ì¶”ì²œ ì´ìœ ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
                "confidence_score": s.get("confidence_score", 50),
                "key_factors": s.get("key_factors", [])
            }
            for s in segments_from_ai if s.get("name")
        }
        
        all_recommendations = []
        for name in [s.get('name') for s in segments_from_ai]:
            seg_data = next((s for s in candidate_segments if s.get('name') == name), None)
            if seg_data:
                seg_copy = seg_data.copy()
                if name in enriched_info_map:
                    seg_copy['reason'] = enriched_info_map[name]['reason']
                    seg_copy['confidence_score'] = float(enriched_info_map[name]['confidence_score'])
                    seg_copy['key_factors'] = enriched_info_map[name]['key_factors']
                all_recommendations.append(seg_copy)

        all_recommendations.sort(key=lambda x: float(x.get('confidence_score', 0)), reverse=True)
        return all_recommendations

    def _get_segments_by_names(self, segment_names: List[str], available_segments: List[Dict]) -> List[Dict]:
        """ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        recommended_segments = []
        available_names_map = {seg['name']: seg for seg in available_segments}
        for name in segment_names:
            if name in available_names_map:
                recommended_segments.append(available_names_map[name].copy())
        return recommended_segments
    
    def _get_priority_segments(self, expanded_keywords: List[str], all_segments_info: List[Dict]) -> (List[Dict], List[Dict]):
        """ 'ìš°ì„  ì¶”ì²œ í›„ë³´'(Aê¸‰)ë¥¼ ì„ ë³„í•˜ê³ , ë‚˜ë¨¸ì§€ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ """
        if not expanded_keywords:
            return [], all_segments_info

        priority_segments = []
        remaining_segments = []
        priority_names = set() 
        lower_keywords = [kw.lower() for kw in expanded_keywords if kw and len(kw) > 1]
        
        if not lower_keywords: 
             return [], all_segments_info

        for segment in all_segments_info:
            found = False
            seg_name = str(segment.get('name', '')).lower()
            seg_desc = str(segment.get('description', '')).lower()
            seg_adv = str(segment.get('recommended_advertisers', '')).lower()
            search_text = f"{seg_name} {seg_desc} {seg_adv}"

            for keyword in lower_keywords:
                if keyword in search_text:
                    original_name = segment.get('name')
                    if original_name not in priority_names:
                        priority_segments.append(segment)
                        priority_names.add(original_name)
                    found = True
                    break 
            
            if not found:
                remaining_segments.append(segment)

        return priority_segments, remaining_segments

    def _extract_db_keywords(self, all_segments_info: List[Dict]) -> Set[str]:
        """ segments.jsonì—ì„œ DB í‚¤ì›Œë“œ ëª©ë¡ì„ ì¶”ì¶œí•˜ëŠ” í—¬í¼ """
        keywords = set()
        for segment in all_segments_info:
            name = segment.get('name')
            if name and pd.notna(name):
                keywords.add(name.strip())
            advertisers = segment.get('recommended_advertisers')
            if advertisers and pd.notna(advertisers):
                split_keywords = re.split(r'[,/\n]', str(advertisers))
                for kw in split_keywords:
                    cleaned_kw = kw.strip()
                    if cleaned_kw and len(cleaned_kw) > 1:
                        keywords.add(cleaned_kw)
        return {kw for kw in keywords if len(kw) > 1}


    def _get_available_segments_info(self) -> List[Dict]:
        """ìƒˆ 4-Depth JSON êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ë„ë¡ ìˆ˜ì •"""
        if 'data' not in self.segments_data or not isinstance(self.segments_data['data'], list):
            return []
            
        segments_info = []
        for segment in self.segments_data['data']:
            if not isinstance(segment, dict):
                continue
            
            cat1 = segment.get('ëŒ€ë¶„ë¥˜')
            cat2 = segment.get('ì¤‘ë¶„ë¥˜')
            cat3 = segment.get('ì†Œë¶„ë¥˜')
            name = segment.get('name', 'N/A')
            
            if cat3 and pd.notna(cat3) and str(cat3).lower() != 'null':
                full_path = f"{cat1} > {cat2} > {cat3} > {name}"
            else:
                full_path = f"{cat1} > {cat2} > {name}"

            seg_copy = segment.copy()
            seg_copy['full_path'] = full_path
            seg_copy['description'] = segment.get('description', '')
            seg_copy['recommended_advertisers'] = segment.get('recommended_advertisers', '')
            
            segments_info.append(seg_copy)
            
        return segments_info
    
    def display_recommendations(self, recommended_segments: List[Dict]):
        """ì¶”ì²œ ê²°ê³¼ í‘œì‹œ"""
        if not recommended_segments:
            st.warning("âŒ ì¶”ì²œí•  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return
        
        st.markdown("""
        <style>
        .tag-box {
            display: inline-block;
            background-color: #28a745;
            color: white;
            padding: 3px 10px;
            border-radius: 15px;
            font-size: 0.9em;
            font-weight: bold;
            margin-right: 5px;
            margin-top: 5px;
            margin-bottom: 5px;
        }
        </style>
        """, unsafe_allow_html=True)

        for i, segment in enumerate(recommended_segments, 1):
            score = float(segment.get('confidence_score', 0)) 
            
            title_text = f"**{i}. {segment.get('full_path', segment.get('name', 'N/A'))}**"
            
            if score <= 60: 
                 title_text += " (ê¸°ë³¸ ì¶”ì²œ)"

            with st.expander(title_text, expanded=True):
                
                if score > 60:
                    st.markdown(f"**ì í•©ë„: <span style='color:#d9534f; font-weight:bold; font-size: 1.1em;'>{score:.0f}ì </span>**", unsafe_allow_html=True)
                    reason_prefix = "ğŸ’¡ AI ì¶”ì²œ ì‚¬ìœ :"
                else:
                    st.markdown(f"**ì í•©ë„:** {score:.0f}ì ")
                    reason_prefix = "â„¹ï¸ ê¸°ë³¸ ì¶”ì²œ ì‚¬ìœ :"
                
                if segment.get('description'):
                    st.write(f"**ğŸ“‹ ì„¤ëª…:** {segment['description']}")

                if segment.get('key_factors'):
                    tags_html = "".join([f"<span class='tag-box'>{factor}</span>" for factor in segment['key_factors']])
                    st.markdown(f"**ğŸ”‘ í•µì‹¬ ë§¤ì¹­ ìš”ì†Œ:** {tags_html}", unsafe_allow_html=True)

                st.divider()

                if segment.get('reason'):
                    if score > 60:
                        st.success(f"**{reason_prefix}** {segment['reason']}")
                    else:
                        st.info(f"**{reason_prefix}** {segment['reason']}")